# çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹å®Œæ•´å¯¹è¯

**æ—¥æœŸ**: 2025-10-11  
**ä¸»é¢˜**: Youtu-GraphRAG çŸ¥è¯†å›¾è°±æ„å»ºä¸æ£€ç´¢æµç¨‹è¯¦è§£

---

## é—®é¢˜1: å½“æˆ‘ä½¿ç”¨webç•Œé¢çš„ Data Uploadï¼Œå¹¶ä¸”ä¸Šä¼ äº†æ•°æ®ï¼Œå¹¶ç‚¹å‡»äº†constructï¼Œæ­¤æ—¶çŸ¥è¯†å›¾è°±æ˜¯å¦‚ä½•æ„å»ºçš„ï¼Œå¸®æˆ‘æ¢³ç†ä¹‹åçš„æ„å»ºæµç¨‹

### å›ç­”

æˆ‘æ¥å¸®ä½ æ¢³ç†ä¸€ä¸‹Webç•Œé¢ä¸Šä¼ æ•°æ®å¹¶ç‚¹å‡»constructåçŸ¥è¯†å›¾è°±çš„æ„å»ºæµç¨‹ã€‚

## ğŸ“Š çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹å®Œæ•´æ¢³ç†

### ğŸ¯ **æµç¨‹æ¦‚è§ˆ**

```
ç”¨æˆ·æ“ä½œ â†’ å‰ç«¯ä¸Šä¼  â†’ åç«¯å¤„ç† â†’ æ„å»ºå›¾è°± â†’ ç¤¾åŒºæ£€æµ‹ â†’ è¿”å›ç»“æœ
```

### ğŸ“ **è¯¦ç»†æµç¨‹**

#### **é˜¶æ®µ1: å‰ç«¯æ•°æ®ä¸Šä¼ ** (`frontend/index.html`)

1. **ç”¨æˆ·æ“ä½œ**ï¼š
   - ç”¨æˆ·åœ¨ "Data Upload" æ ‡ç­¾é¡µä¸Šä¼ æ–‡ä»¶ï¼ˆæ”¯æŒ `.json`, `.txt` ç­‰æ ¼å¼ï¼‰
   - ç‚¹å‡» "Upload Files" æŒ‰é’®

2. **ä¸Šä¼ è¯·æ±‚**ï¼š
   ```javascript
   POST /api/upload
   - æ–‡ä»¶é€šè¿‡ FormData ä¸Šä¼ 
   - æºå¸¦ client_id ç”¨äº WebSocket è¿æ¥
   ```

3. **æ–‡ä»¶å¤„ç†**ï¼š
   - å‰ç«¯å°†æ–‡ä»¶ä¿å­˜åˆ° `data/uploaded/{dataset_name}/` ç›®å½•
   - è§£ææ–‡ä»¶å†…å®¹ä¸º `corpus.json` æ ¼å¼
   - çŠ¶æ€æ˜¾ç¤ºä¸º "needs_construction"

---

#### **é˜¶æ®µ2: ç‚¹å‡» Construct æŒ‰é’®**

1. **å‰ç«¯å‘èµ·æ„å»ºè¯·æ±‚**ï¼š
   ```javascript
   POST /api/construct-graph
   {
     "dataset_name": "xxx",
     "client_id": "web_client"
   }
   ```

2. **å»ºç«‹ WebSocket è¿æ¥**ï¼š
   ```javascript
   ws://localhost:8000/ws/web_client
   // ç”¨äºæ¥æ”¶å®æ—¶æ„å»ºè¿›åº¦æ›´æ–°
   ```

---

#### **é˜¶æ®µ3: åç«¯æ„å»ºå¤„ç†** (`backend.py`)

**3.1 æ¸…ç†ç¼“å­˜** (Progress: 2%-5%)
```python
await clear_cache_files(dataset_name)
```
- åˆ é™¤ FAISS ç´¢å¼•ç¼“å­˜ï¼š`retriever/faiss_cache_new/{dataset_name}/`
- åˆ é™¤æ—§çš„ chunks æ–‡ä»¶ï¼š`output/chunks/{dataset_name}.txt`
- åˆ é™¤æ—§çš„å›¾è°±æ–‡ä»¶ï¼š`output/graphs/{dataset_name}_new.json`

**3.2 åˆå§‹åŒ–é…ç½®** (Progress: 5%-10%)
```python
config = get_config("config/base_config.yaml")
schema_path = "schemas/demo.json"
corpus_path = f"data/uploaded/{dataset_name}/corpus.json"
```

**3.3 åˆ›å»º KTBuilder å®ä¾‹** (Progress: 10%-20%)
```python
builder = constructor.KTBuilder(
    dataset_name,
    schema_path,
    mode=config.construction.mode,  # "agent" æˆ– "noagent"
    config=config
)
```

---

#### **é˜¶æ®µ4: çŸ¥è¯†å›¾è°±æ„å»º** (`models/constructor/kt_gen.py`)

**4.1 åŠ è½½è¯­æ–™åº“** (Progress: 20%-30%)
```python
def build_knowledge_graph(self, corpus):
    with open(corpus, 'r') as f:
        documents = json.load(f)  # åŠ è½½æ‰€æœ‰æ–‡æ¡£
```

**4.2 å¹¶è¡Œå¤„ç†æ–‡æ¡£** (Progress: 30%-85%)
```python
def process_all_documents(self, documents):
    max_workers = min(config.max_workers, cpu_count + 4)
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ä¸ºæ¯ä¸ªæ–‡æ¡£æäº¤å¤„ç†ä»»åŠ¡
        futures = [executor.submit(self.process_document, doc) 
                  for doc in documents]
```

**4.2.1 å•ä¸ªæ–‡æ¡£å¤„ç†** (`process_document`)

1. **æ–‡æœ¬åˆ†å—**ï¼š
   ```python
   chunks, chunk2id = self.chunk_text(doc)
   # ç”Ÿæˆå”¯ä¸€çš„ chunk_id (8ä½ nanoid)
   ```

2. **LLM ä¿¡æ¯æŠ½å–** (æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒå¤„ç†æ–¹æ³•)ï¼š

   **å¦‚æœæ˜¯ agent æ¨¡å¼**ï¼š
   ```python
   process_level1_level2_agent(chunk, id)
   ```
   - è°ƒç”¨ LLM API è¿›è¡Œå®ä½“ã€å…³ç³»ã€å±æ€§æŠ½å–
   - **æ”¯æŒ Schema Evolution**ï¼ˆæ¨¡å¼æ¼”åŒ–ï¼‰
   - æå–å†…å®¹åŒ…æ‹¬ï¼š
     - `attributes`: å®ä½“å±æ€§ `{"å®ä½“å": ["å±æ€§1", "å±æ€§2"]}`
     - `triples`: ä¸‰å…ƒç»„ `[ä¸»ä½“, å…³ç³», å®¢ä½“]`
     - `entity_types`: å®ä½“ç±»å‹æ˜ å°„
     - `new_schema_types`: æ–°å‘ç°çš„ schema ç±»å‹ï¼ˆä»… agent æ¨¡å¼ï¼‰

   **å¦‚æœæ˜¯ noagent æ¨¡å¼**ï¼š
   ```python
   process_level1_level2(chunk, id)
   ```
   - ä»…è¿›è¡Œæ ‡å‡†æŠ½å–ï¼Œä¸æ”¯æŒæ¨¡å¼æ¼”åŒ–

3. **æ„å»ºå›¾ç»“æ„**ï¼š
   ```python
   # Level 1: å±æ€§èŠ‚ç‚¹
   attr_node_id = f"attr_{counter}"
   graph.add_node(attr_node_id, 
                  label="attribute",
                  properties={"name": attr, "chunk id": chunk_id},
                  level=1)
   
   # Level 2: å®ä½“èŠ‚ç‚¹
   entity_node_id = f"entity_{counter}"
   graph.add_node(entity_node_id,
                  label="entity", 
                  properties={"name": entity, "schema_type": type},
                  level=2)
   
   # æ·»åŠ è¾¹ (å…³ç³»)
   graph.add_edge(subj_id, obj_id, relation=predicate)
   ```

**4.3 ä¸‰å…ƒç»„å»é‡** (Progress: 85%-90%)
```python
def triple_deduplicate(self):
    # å»é™¤é‡å¤çš„ä¸‰å…ƒç»„å…³ç³»
    seen_triples = set()
    for u, v, data in graph.edges(data=True):
        if (u, v, relation) not in seen_triples:
            seen_triples.add((u, v, relation))
            new_graph.add_edge(u, v, **data)
```

**4.4 ç¤¾åŒºæ£€æµ‹ (Level 4)** (Progress: 90%-95%)
```python
def process_level4(self):
    # ä½¿ç”¨ FastTreeComm ç®—æ³•è¿›è¡Œç¤¾åŒºæ£€æµ‹
    _tree_comm = tree_comm.FastTreeComm(
        self.graph,
        embedding_model="all-MiniLM-L6-v2",
        struct_weight=0.3
    )
    
    # æ£€æµ‹å®ä½“èŠ‚ç‚¹çš„ç¤¾åŒºç»“æ„
    level2_nodes = [n for n in graph.nodes if level == 2]
    comm_to_nodes = _tree_comm.detect_communities(level2_nodes)
    
    # åˆ›å»ºè¶…çº§èŠ‚ç‚¹ (Level 4 ç¤¾åŒº)
    _tree_comm.create_super_nodes_with_keywords(comm_to_nodes, level=4)
```

**ç¤¾åŒºæ£€æµ‹åŸç†**ï¼š
- åŸºäºç»“æ„å’Œè¯­ä¹‰ç›¸ä¼¼åº¦èšç±»å®ä½“
- ä¸ºæ¯ä¸ªç¤¾åŒºç”Ÿæˆå…³é”®è¯æ ‡ç­¾
- æ„å»ºå±‚æ¬¡åŒ–çš„çŸ¥è¯†å›¾è°±ç»“æ„

**4.5 ä¿å­˜ç»“æœ** (Progress: 95%-100%)

1. **ä¿å­˜ Chunks**ï¼š
   ```python
   # ä¿å­˜åˆ° output/chunks/{dataset_name}.txt
   f.write(f"id: {chunk_id}\tChunk: {chunk_text}\n")
   ```

2. **æ ¼å¼åŒ–è¾“å‡º**ï¼š
   ```python
   def format_output(self):
       output = []
       for u, v, data in graph.edges(data=True):
           relationship = {
               "start_node": {
                   "label": u_data["label"],
                   "properties": u_data["properties"]
               },
               "relation": data["relation"],
               "end_node": {
                   "label": v_data["label"],
                   "properties": v_data["properties"]
               }
           }
           output.append(relationship)
       return output
   ```

3. **ä¿å­˜å›¾è°±**ï¼š
   ```python
   # ä¿å­˜åˆ° output/graphs/{dataset_name}_new.json
   json.dump(output, f, ensure_ascii=False, indent=2)
   ```

---

#### **é˜¶æ®µ5: å‡†å¤‡å¯è§†åŒ–æ•°æ®**

```python
graph_vis_data = await prepare_graph_visualization(graph_path)
```

**è½¬æ¢ä¸º ECharts æ ¼å¼**ï¼š
```javascript
{
  "nodes": [
    {
      "id": "entity_1",
      "name": "å®ä½“åç§°",
      "category": "person",
      "symbolSize": 25
    }
  ],
  "links": [
    {
      "source": "entity_1",
      "target": "entity_2",
      "name": "å…³ç³»åç§°"
    }
  ],
  "categories": [...]
}
```

---

#### **é˜¶æ®µ6: è¿”å›å‰ç«¯**

1. **æ›´æ–°æ•°æ®é›†çŠ¶æ€**ï¼š
   ```
   status: "needs_construction" â†’ "ready"
   ```

2. **WebSocket æ¨é€å®Œæˆæ¶ˆæ¯**ï¼š
   ```json
   {
     "type": "complete",
     "message": "å›¾æ„å»ºå®Œæˆ!"
   }
   ```

3. **å‰ç«¯æ˜¾ç¤º**ï¼š
   - æ•°æ®é›†åˆ—è¡¨ä¸­çŠ¶æ€å˜ä¸º "ready"
   - æ˜¾ç¤º "Reconstruct" å’Œ "Delete" æŒ‰é’®
   - å¯ä»¥åœ¨ Graph Visualization ä¸­æŸ¥çœ‹å›¾è°±
   - å¯ä»¥åœ¨ Q&A Interface ä¸­è¿›è¡Œæ£€ç´¢é—®ç­”

---

### ğŸ”‘ **å…³é”®æŠ€æœ¯ç‚¹**

1. **å¹¶å‘å¤„ç†**ï¼šä½¿ç”¨ ThreadPoolExecutor æœ€å¤š32ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†æ–‡æ¡£
2. **LLM æŠ½å–**ï¼šè°ƒç”¨ LLM API è¿›è¡Œç»“æ„åŒ–ä¿¡æ¯æŠ½å–
3. **Schema Evolution** (Agentæ¨¡å¼)ï¼šåŠ¨æ€æ‰©å±• schema ç±»å‹
4. **å›¾ç»“æ„**ï¼šä½¿ç”¨ NetworkX MultiDiGraph æ„å»ºå¤šå±‚å›¾ç»“æ„
   - Level 1: å±æ€§èŠ‚ç‚¹
   - Level 2: å®ä½“èŠ‚ç‚¹
   - Level 4: ç¤¾åŒºèŠ‚ç‚¹
5. **ç¤¾åŒºæ£€æµ‹**ï¼šFastTreeComm ç®—æ³•åŸºäºç»“æ„+è¯­ä¹‰è¿›è¡Œèšç±»
6. **å®æ—¶è¿›åº¦**ï¼šé€šè¿‡ WebSocket å‘å‰ç«¯æ¨é€æ„å»ºè¿›åº¦

---

### ğŸ“ **è¾“å‡ºæ–‡ä»¶**

æ„å»ºå®Œæˆåä¼šç”Ÿæˆï¼š
- `output/chunks/{dataset_name}.txt` - æ–‡æœ¬å—å­˜å‚¨
- `output/graphs/{dataset_name}_new.json` - çŸ¥è¯†å›¾è°±æ•°æ®
- `retriever/faiss_cache_new/{dataset_name}/` - FAISS ç´¢å¼•ç¼“å­˜ï¼ˆæ£€ç´¢æ—¶ç”Ÿæˆï¼‰

è¿™å°±æ˜¯å®Œæ•´çš„çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹ï¼ğŸ‰

---

## é—®é¢˜2: 3.3 åˆ›å»º KTBuilder å®ä¾‹ (Progress: 10%-20%) è¿™ä¸ªé»˜è®¤æ˜¯ä»€ä¹ˆæ¨¡å¼

### å›ç­”

## ğŸ¯ **é»˜è®¤æ¨¡å¼æ˜¯ `agent`**

æ ¹æ®é…ç½®æ–‡ä»¶ `config/base_config.yaml`ï¼š

```yaml
construction:
  mode: agent        # ğŸ‘ˆ æ„å»ºæ¨¡å¼é»˜è®¤ä¸º agent
  max_workers: 32
  chunk_size: 1000
  overlap: 200

triggers:
  constructor_trigger: true
  mode: agent        # ğŸ‘ˆ å…¨å±€æ¨¡å¼ä¹Ÿæ˜¯ agent
  retrieve_trigger: true
```

åœ¨ `KTBuilder` åˆå§‹åŒ–æ—¶ï¼š

```python
def __init__(self, dataset_name, schema_path=None, mode=None, config=None):
    # ...
    self.mode = mode or config.construction.mode  # å¦‚æœæ²¡ä¼  modeï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶çš„ agent
```

---

## ğŸ“Š **ä¸¤ç§æ¨¡å¼çš„åŒºåˆ«**

### **ğŸ¤– Agent æ¨¡å¼** (é»˜è®¤)

**ç‰¹ç‚¹**ï¼š
- âœ… **æ”¯æŒ Schema Evolutionï¼ˆæ¨¡å¼æ¼”åŒ–ï¼‰**
- âœ… LLM å¯ä»¥åŠ¨æ€å‘ç°å¹¶æ·»åŠ æ–°çš„å®ä½“ç±»å‹ã€å…³ç³»ç±»å‹ã€å±æ€§ç±»å‹
- âœ… æ›´çµæ´»ï¼Œé€‚åˆå¼€æ”¾åŸŸçŸ¥è¯†æŠ½å–
- âœ… ä½¿ç”¨ `process_level1_level2_agent()` æ–¹æ³•

**LLM Prompt åŒ…å«**ï¼š
```
Schema Evolution: If you find new and important entity types, 
relation types, or attribute types that are valuable for knowledge 
extraction, include them in a "new_schema_types" field.
```

**è¿”å›æ ¼å¼**ï¼š
```json
{
  "attributes": {...},
  "triples": [...],
  "entity_types": {...},
  "new_schema_types": {      // ğŸ‘ˆ æ–°å¢å­—æ®µ
    "nodes": ["æ–°å®ä½“ç±»å‹"],
    "relations": ["æ–°å…³ç³»ç±»å‹"],
    "attributes": ["æ–°å±æ€§ç±»å‹"]
  }
}
```

---

### **ğŸ“ NoAgent æ¨¡å¼**

**ç‰¹ç‚¹**ï¼š
- âŒ **ä¸æ”¯æŒ Schema Evolution**
- âœ… ä¸¥æ ¼æŒ‰ç…§é¢„å®šä¹‰ schema æŠ½å–
- âœ… æ›´å¯æ§ï¼Œé€‚åˆå°é—­åŸŸçŸ¥è¯†æŠ½å–
- âœ… ä½¿ç”¨ `process_level1_level2()` æ–¹æ³•

**è¿”å›æ ¼å¼**ï¼š
```json
{
  "attributes": {...},
  "triples": [...],
  "entity_types": {...}
  // æ²¡æœ‰ new_schema_types å­—æ®µ
}
```

---

## ğŸ”§ **å¦‚ä½•åˆ‡æ¢æ¨¡å¼ï¼Ÿ**

### **æ–¹æ³•1: ä¿®æ”¹é…ç½®æ–‡ä»¶**
ç¼–è¾‘ `config/base_config.yaml`ï¼š
```yaml
construction:
  mode: noagent  # æ”¹ä¸º noagent
```

### **æ–¹æ³•2: é€šè¿‡å‘½ä»¤è¡Œå‚æ•°**
```bash
python main.py --datasets demo --override '{"construction": {"mode": "noagent"}}'
```

### **æ–¹æ³•3: åœ¨ä»£ç ä¸­æŒ‡å®š**
```python
builder = constructor.KTBuilder(
    dataset_name,
    schema_path,
    mode="noagent",  # æ˜¾å¼æŒ‡å®š
    config=config
)
```

---

## ğŸ’¡ **æ¨¡å¼é€‰æ‹©å»ºè®®**

| åœºæ™¯ | æ¨èæ¨¡å¼ | åŸå›  |
|------|---------|------|
| é€šç”¨æ–‡æ¡£å¤„ç† | **agent** | å¯ä»¥è‡ªåŠ¨å‘ç°æ–°ç±»å‹ |
| ç‰¹å®šé¢†åŸŸï¼ˆå¦‚å­¦æœ¯è®ºæ–‡ï¼‰ | **agent** | é¢†åŸŸæœ¯è¯­å¯èƒ½ä¸åœ¨é¢„å®šä¹‰ schema ä¸­ |
| ä¸¥æ ¼ç»“æ„åŒ–æ•°æ® | **noagent** | ä¿æŒ schema ä¸€è‡´æ€§ |
| å·²æœ‰å®Œæ•´ schema | **noagent** | ä¸éœ€è¦æ‰©å±• schema |

**é»˜è®¤ä½¿ç”¨ agent æ¨¡å¼æ˜¯åˆç†çš„**ï¼Œå› ä¸ºå®ƒæ›´çµæ´»ï¼Œé€‚åˆå¤„ç†å„ç§ç±»å‹çš„æ–‡æ¡£ï¼ğŸ¯

---

## é—®é¢˜3: ç»§ç»­

### å›ç­”

å¥½çš„ï¼è®©æˆ‘ç»§ç»­æ·±å…¥è®²è§£ **æ£€ç´¢å’Œé—®ç­”æµç¨‹**ï¼Œè¿™æ˜¯æ„å»ºçŸ¥è¯†å›¾è°±åçš„å…³é”®åº”ç”¨ç¯èŠ‚ï¼

---

## ğŸ” **çŸ¥è¯†å›¾è°±æ£€ç´¢ä¸é—®ç­”æµç¨‹**

### **åœºæ™¯ï¼šç”¨æˆ·åœ¨ Q&A Interface æé—®**

å½“ç”¨æˆ·åœ¨ Web ç•Œé¢çš„ "Q&A Interface" è¾“å…¥é—®é¢˜å¹¶ç‚¹å‡»æäº¤åï¼Œç³»ç»Ÿä¼šè¿›è¡Œä»¥ä¸‹å¤„ç†ï¼š

---

## ğŸ“‹ **æ£€ç´¢æµç¨‹æ€»è§ˆ**

```
ç”¨æˆ·æé—® â†’ é—®é¢˜åˆ†è§£ â†’ åˆå§‹æ£€ç´¢ â†’ IRCoTè¿­ä»£æ¨ç† â†’ ç­”æ¡ˆç”Ÿæˆ â†’ è¿”å›ç»“æœ
```

---

## ğŸš€ **è¯¦ç»†æµç¨‹ï¼ˆAgent æ¨¡å¼ï¼‰**

### **Step 1: å‰ç«¯å‘èµ·è¯·æ±‚**

```javascript
POST /api/ask-question
{
  "question": "When was the person who Messi's goals compared to get signed by Barcelona?",
  "dataset_name": "demo",
  "client_id": "web_client"
}
```

---

### **Step 2: åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ** (Progress: 10%)

```python
# backend.py - ask_question()
graphq = decomposer.GraphQ(dataset_name, config=config)

kt_retriever = retriever.KTRetriever(
    dataset_name,
    graph_path,
    recall_paths=config.retrieval.recall_paths,  # é»˜è®¤ 2
    schema_path=schema_path,
    top_k=config.retrieval.top_k_filter,  # é»˜è®¤ 20
    mode="agent",  # å¼ºåˆ¶ agent æ¨¡å¼
    config=config
)
```

---

### **Step 3: æ„å»º FAISS ç´¢å¼•** (Progress: 10%-40%)

```python
kt_retriever.build_indices()
```

**ç´¢å¼•ç±»å‹**ï¼š
1. **Node Index** - å®ä½“èŠ‚ç‚¹ç´¢å¼•
   - å¯¹æ‰€æœ‰ Level 2 å®ä½“èŠ‚ç‚¹çš„åç§°è¿›è¡ŒåµŒå…¥
   - ä½¿ç”¨ FAISS IndexFlatIP (å†…ç§¯ç›¸ä¼¼åº¦)

2. **Relation Index** - å…³ç³»ç´¢å¼•
   - å¯¹æ‰€æœ‰è¾¹çš„å…³ç³»è¿›è¡ŒåµŒå…¥

3. **Chunk Index** - æ–‡æœ¬å—ç´¢å¼•
   - å¯¹åŸå§‹æ–‡æœ¬å—è¿›è¡ŒåµŒå…¥

4. **Community Index** - ç¤¾åŒºç´¢å¼•
   - å¯¹ Level 4 ç¤¾åŒºèŠ‚ç‚¹çš„å…³é”®è¯è¿›è¡ŒåµŒå…¥

**ç¼“å­˜æœºåˆ¶**ï¼š
```python
# ç´¢å¼•ç¼“å­˜åœ¨ retriever/faiss_cache_new/{dataset_name}/
cache_files = [
    "node.index",
    "node_embeddings.npy",
    "node_map.json",
    "chunk_embedding_cache.pt",
    "comm.index",
    "comm_map.json"
]
```

---

### **Step 4: é—®é¢˜åˆ†è§£** (Progress: 40%-50%)

```python
decomposition = graphq.decompose(question, schema_path)
```

**LLM Prompt** (ç®€åŒ–ç‰ˆ):
```
You are a question decomposition expert.
Given the question and ontology, decompose it into 2-3 sub-questions.

Ontology: {schema}
Question: {question}

Return JSON:
{
  "sub_questions": [
    {"sub-question": "Who did Messi compare to?"},
    {"sub-question": "When was that person signed by Barcelona?"}
  ],
  "involved_types": {
    "nodes": ["person", "organization"],
    "relations": ["comparable_to", "signed_by"],
    "attributes": ["name", "date"]
  }
}
```

**è¿”å›ç»“æœ**ï¼š
```python
{
  "sub_questions": [
    {"sub-question": "Who did Messi's goals in Copa del Rey compare to?"},
    {"sub-question": "When was that person signed by Barcelona?"}
  ],
  "involved_types": {
    "nodes": ["person", "organization", "event"],
    "relations": ["comparable_to", "signed_by"],
    "attributes": ["date"]
  }
}
```

---

### **Step 5: å¹¶è¡Œå­é—®é¢˜æ£€ç´¢** (Progress: 50%-65%)

```python
# å¦‚æœæœ‰å¤šä¸ªå­é—®é¢˜ï¼Œä½¿ç”¨å¹¶è¡Œå¤„ç†
if len(sub_questions) > 1:
    aggregated_results, elapsed = kt_retriever.process_subquestions_parallel(
        sub_questions, 
        top_k=20,
        involved_types=involved_types
    )
```

**æ¯ä¸ªå­é—®é¢˜çš„æ£€ç´¢è¿‡ç¨‹**ï¼š

#### **5.1 å¤šè·¯å¬å› (Multi-Path Recall)**

```python
def process_retrieval_results(self, query, top_k, involved_types):
    all_triples = set()
    all_chunk_ids = set()
    
    # Path 1: èŠ‚ç‚¹å¬å› (Node Recall)
    node_results = self._recall_from_nodes(query, top_k)
    all_triples.update(node_results['triples'])
    
    # Path 2: ç¤¾åŒºå¬å› (Community Recall)
    comm_results = self._recall_from_communities(query, top_k)
    all_triples.update(comm_results['triples'])
    
    # Path 3: Chunk å¬å› (Chunk Recall) - å¦‚æœå¯ç”¨
    if config.retrieval.enable_high_recall:
        chunk_results = self._recall_from_chunks(query, top_k)
        all_chunk_ids.update(chunk_results['chunk_ids'])
    
    return {
        'triples': list(all_triples),
        'chunk_ids': list(all_chunk_ids),
        'chunk_contents': chunk_contents_dict
    }
```

**Path 1: èŠ‚ç‚¹å¬å›è¯¦è§£**

```python
def _recall_from_nodes(self, query, top_k):
    # 1. æŸ¥è¯¢åµŒå…¥
    query_embedding = self.embedding_model.encode(query)
    
    # 2. FAISS ç›¸ä¼¼åº¦æœç´¢
    distances, indices = self.node_index.search(
        query_embedding.reshape(1, -1), 
        k=top_k * 2  # å¬å›2å€å€™é€‰
    )
    
    # 3. è·å–åŒ¹é…çš„èŠ‚ç‚¹
    matched_nodes = [self.node_id_map[idx] for idx in indices[0]]
    
    # 4. æå–ç›¸å…³ä¸‰å…ƒç»„
    triples = []
    for node_id in matched_nodes:
        # è·å–è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰å‡ºè¾¹å’Œå…¥è¾¹
        for u, v, data in self.graph.edges(node_id, data=True):
            triple = f"({u_name}, {relation}, {v_name})"
            triples.append(triple)
    
    return {'triples': triples}
```

**Path 2: ç¤¾åŒºå¬å›è¯¦è§£**

```python
def _recall_from_communities(self, query, top_k):
    # 1. åœ¨ç¤¾åŒºç´¢å¼•ä¸­æœç´¢
    distances, indices = self.comm_index.search(
        query_embedding.reshape(1, -1), 
        k=min(5, num_communities)  # å¬å›æœ€ç›¸å…³çš„5ä¸ªç¤¾åŒº
    )
    
    # 2. è·å–ç¤¾åŒºå†…çš„æ‰€æœ‰å®ä½“
    community_nodes = []
    for comm_id in matched_communities:
        # ç¤¾åŒºèŠ‚ç‚¹è¿æ¥çš„æ‰€æœ‰å®ä½“
        members = self.graph.neighbors(comm_id)
        community_nodes.extend(members)
    
    # 3. æå–ç¤¾åŒºå†…çš„ä¸‰å…ƒç»„
    triples = self._extract_triples_from_nodes(community_nodes)
    
    return {'triples': triples}
```

**æ£€ç´¢ç»“æœç¤ºä¾‹**ï¼š
```python
{
  'triples': [
    "(Messi [schema_type: person], comparable_to, Cristiano Ronaldo [schema_type: person])",
    "(Cristiano Ronaldo [schema_type: person], signed_by, Barcelona [schema_type: organization])",
    "(Cristiano Ronaldo [schema_type: person], has_attribute, signed_date: 2003)",
  ],
  'chunk_ids': ['AbCd1234', 'EfGh5678'],
  'chunk_contents': {
    'AbCd1234': "Messi's goals in Copa del Rey are often compared to...",
    'EfGh5678': "Cristiano Ronaldo was signed by Barcelona in 2003..."
  }
}
```

---

### **Step 6: èšåˆå­é—®é¢˜ç»“æœ**

```python
all_triples = set()
all_chunk_ids = set()

for sub_q_result in sub_question_results:
    all_triples.update(sub_q_result['triples'])
    all_chunk_ids.update(sub_q_result['chunk_ids'])

# å»é‡
dedup_triples = list(all_triples)  # çº¦50-100æ¡
dedup_chunks = list(all_chunk_ids)  # çº¦10-20ä¸ª
```

---

### **Step 7: ç”Ÿæˆåˆå§‹ç­”æ¡ˆ** (Progress: 65%)

```python
# æ„å»ºä¸Šä¸‹æ–‡
context = "=== Triples ===\n" + "\n".join(dedup_triples[:20])
context += "\n=== Chunks ===\n" + "\n".join(dedup_chunks[:10])

# ç”Ÿæˆæç¤ºè¯
prompt = f"""
You are an expert knowledge assistant.
Question: {question}
Knowledge Context:
{context}

Answer (be specific and direct):
"""

# è°ƒç”¨ LLM
initial_answer = kt_retriever.generate_answer(prompt)
# ç»“æœ: "Cristiano Ronaldo was signed by Barcelona in 2003."
```

---

### **Step 8: IRCoT è¿­ä»£æ¨ç†** (Progress: 65%-90%)

**IRCoT (Iterative Retrieval Chain-of-Thought)**ï¼šé€šè¿‡å¤šè½®è¿­ä»£ï¼Œé€æ­¥å®Œå–„ç­”æ¡ˆ

```python
max_steps = 5  # æœ€å¤š5è½®è¿­ä»£
current_query = question
thoughts = [initial_answer]  # è®°å½•æ¨ç†å†å²

for step in range(1, max_steps + 1):
    # 8.1 ç”Ÿæˆæ¨ç†æç¤º
    ircot_prompt = f"""
    You are using iterative retrieval with chain-of-thought reasoning.
    
    Current Question: {question}
    Current Query: {current_query}
    
    Available Knowledge:
    {context}
    
    Previous Thoughts: {' | '.join(thoughts)}
    
    Instructions:
    1. If you have enough info, write "So the answer is: <answer>"
    2. If you need more info, write "The new query is: <query>"
    
    Your reasoning:
    """
    
    # 8.2 LLM æ¨ç†
    reasoning = kt_retriever.generate_answer(ircot_prompt)
    thoughts.append(reasoning)
    
    # 8.3 åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­æ£€ç´¢
    if "So the answer is:" in reasoning:
        # æ‰¾åˆ°æœ€ç»ˆç­”æ¡ˆï¼Œé€€å‡ºå¾ªç¯
        final_answer = extract_answer(reasoning)
        break
    
    if "The new query is:" in reasoning:
        # æå–æ–°æŸ¥è¯¢
        new_query = extract_query(reasoning)
        current_query = new_query
        
        # 8.4 æ‰§è¡Œæ–°ä¸€è½®æ£€ç´¢
        new_results = kt_retriever.process_retrieval_results(
            new_query, 
            top_k=20
        )
        
        # 8.5 åˆå¹¶æ–°çŸ¥è¯†
        all_triples.update(new_results['triples'])
        all_chunk_ids.update(new_results['chunk_ids'])
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        context = rebuild_context(all_triples, all_chunk_ids)
    else:
        # æ²¡æœ‰æ˜ç¡®æŒ‡ä»¤ï¼Œä½¿ç”¨åˆå§‹ç­”æ¡ˆ
        break
```

**IRCoT ç¤ºä¾‹å¯¹è¯**ï¼š

```
Step 1:
Reasoning: "I found that Messi is compared to Cristiano Ronaldo, 
           but I need to verify when Ronaldo was signed by Barcelona.
           The new query is: When did Cristiano Ronaldo sign with Barcelona?"

Step 2: (æ£€ç´¢æ–°ä¿¡æ¯)
Retrieved: "(Cristiano Ronaldo, signed_by, Barcelona), (date: 2003-07-01)"

Step 3:
Reasoning: "Based on the retrieved information, Cristiano Ronaldo 
           was signed by Barcelona on July 1, 2003.
           So the answer is: 2003"
```

---

### **Step 9: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ** (Progress: 90%-100%)

```python
# ä½¿ç”¨æ‰€æœ‰æ”¶é›†çš„çŸ¥è¯†ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
final_context = "=== Final Triples ===\n" + "\n".join(all_triples)
final_context += "\n=== Final Chunks ===\n" + "\n".join(all_chunks)

final_prompt = kt_retriever.generate_prompt(question, final_context)
final_answer = kt_retriever.generate_answer(final_prompt)
```

---

### **Step 10: å‡†å¤‡å¯è§†åŒ–æ•°æ®**

```python
visualization_data = {
    "subqueries": prepare_subquery_visualization(sub_questions),
    "knowledge_graph": prepare_retrieved_graph_visualization(triples),
    "reasoning_flow": prepare_reasoning_flow_visualization(reasoning_steps),
    "retrieval_details": {
        "total_triples": len(triples),
        "total_chunks": len(chunks),
        "sub_questions_count": len(sub_questions),
        "ircot_steps": len(thoughts)
    }
}
```

---

### **Step 11: è¿”å›å‰ç«¯**

```python
return QuestionResponse(
    answer=final_answer,  # "2003"
    sub_questions=sub_questions,
    retrieved_triples=triples[:20],  # å‰20æ¡ä¸‰å…ƒç»„
    retrieved_chunks=chunks[:10],  # å‰10ä¸ªæ–‡æœ¬å—
    reasoning_steps=reasoning_steps,  # IRCoTæ¨ç†æ­¥éª¤
    visualization_data=visualization_data
)
```

---

## ğŸ¨ **å‰ç«¯å±•ç¤º**

### **1. ç­”æ¡ˆåŒºåŸŸ**
```
Answer: 2003
```

### **2. æ£€ç´¢ç»Ÿè®¡**
```
ğŸ“Š Retrieval Stats
- Sub-questions: 2
- Retrieved Triples: 47
- Relevant Chunks: 12
```

### **3. é—®é¢˜åˆ†è§£**
```
ğŸ“ Question Decomposition

1ï¸âƒ£ Who did Messi's goals in Copa del Rey compare to?
   Triples: 23 | Chunks: 6 | Time: 0.45s

2ï¸âƒ£ When was that person signed by Barcelona?
   Triples: 24 | Chunks: 6 | Time: 0.38s
```

### **4. å­å›¾å¯è§†åŒ–**
- ä½¿ç”¨ ECharts ç»˜åˆ¶æ£€ç´¢åˆ°çš„ä¸‰å…ƒç»„å­å›¾
- èŠ‚ç‚¹ï¼šå®ä½“ï¼ˆå¸¦ç±»å‹é¢œè‰²ç¼–ç ï¼‰
- è¾¹ï¼šå…³ç³»ï¼ˆå¸¦æ ‡ç­¾ï¼‰

---

## âš¡ **æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**

### **1. å¹¶è¡Œå¤„ç†**
```python
# å­é—®é¢˜å¹¶è¡Œæ£€ç´¢
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(process_subquestion, sq) 
               for sq in sub_questions]
    results = [f.result() for f in futures]
```

### **2. ç¼“å­˜æœºåˆ¶**
- FAISS ç´¢å¼•ç¼“å­˜åœ¨ç£ç›˜
- Embedding ç¼“å­˜é¿å…é‡å¤è®¡ç®—
- LRU ç¼“å­˜å¸¸è§æŸ¥è¯¢ç»“æœ

### **3. æ—©åœç­–ç•¥**
- IRCoT åœ¨æ‰¾åˆ°ç­”æ¡ˆåç«‹å³åœæ­¢
- ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼æ—¶ä¸å¬å›

### **4. Top-K æˆªæ–­**
- æ¯è·¯å¬å›æœ€å¤š Top-20
- æœ€ç»ˆè¾“å‡ºæœ€å¤š 20 æ¡ä¸‰å…ƒç»„ã€10 ä¸ª chunks

---

## ğŸ“Š **Agent vs NoAgent æ¨¡å¼å¯¹æ¯”**

| ç‰¹æ€§ | Agent æ¨¡å¼ | NoAgent æ¨¡å¼ |
|------|-----------|-------------|
| é—®é¢˜åˆ†è§£ | âœ… æ”¯æŒ | âœ… æ”¯æŒ |
| å¹¶è¡Œå­é—®é¢˜å¤„ç† | âœ… æ”¯æŒ | âœ… æ”¯æŒ |
| **IRCoT è¿­ä»£æ¨ç†** | âœ… **å¯ç”¨** | âŒ **ç¦ç”¨** |
| å¤šè½®æ£€ç´¢ | âœ… æœ€å¤š5è½® | âŒ åªæœ‰1è½® |
| æ¨ç†é“¾ | âœ… è®°å½•å®Œæ•´æ€è€ƒè¿‡ç¨‹ | âŒ ç›´æ¥ç”Ÿæˆç­”æ¡ˆ |
| é€‚ç”¨åœºæ™¯ | å¤æ‚å¤šè·³æ¨ç† | ç®€å•äº‹å®æŸ¥è¯¢ |
| å¹³å‡è€—æ—¶ | ~3-5ç§’ | ~1-2ç§’ |

---

## ğŸ¯ **æ€»ç»“**

å®Œæ•´çš„æ£€ç´¢æµç¨‹ï¼š

```
é—®é¢˜è¾“å…¥
  â†“
é—®é¢˜åˆ†è§£ (2-3ä¸ªå­é—®é¢˜)
  â†“
å¹¶è¡Œæ£€ç´¢ (å¤šè·¯å¬å›: Node + Community + Chunk)
  â†“
åˆå§‹ç­”æ¡ˆç”Ÿæˆ
  â†“
IRCoT è¿­ä»£æ¨ç† (æœ€å¤š5è½®)
  â”œâ”€ åˆ¤æ–­æ˜¯å¦è¶³å¤Ÿ
  â”œâ”€ ç”Ÿæˆæ–°æŸ¥è¯¢
  â”œâ”€ æ‰§è¡Œæ–°æ£€ç´¢
  â””â”€ æ›´æ–°çŸ¥è¯†åº“
  â†“
æœ€ç»ˆç­”æ¡ˆç”Ÿæˆ
  â†“
å¯è§†åŒ–å‘ˆç°
```

è¿™å°±æ˜¯å®Œæ•´çš„çŸ¥è¯†å›¾è°±æ£€ç´¢å’Œé—®ç­”æµç¨‹ï¼ğŸš€

---

## é—®é¢˜4: agentæ¨¡å¼ä¸‹ï¼Œå‘ç°çš„æ–°çš„å®ä½“å’Œå…³ç³»ï¼Œåç»­æœ‰ä»€ä¹ˆå¤„ç†

### å›ç­”

## ğŸ”„ **Agent æ¨¡å¼ä¸‹æ–° Schema ç±»å‹çš„å®Œæ•´å¤„ç†æµç¨‹**

---

## ğŸ“Š **æµç¨‹æ¦‚è§ˆ**

```
LLM å‘ç°æ–°ç±»å‹ â†’ å»é‡æ£€æŸ¥ â†’ æ›´æ–° Schema æ–‡ä»¶ â†’ æ›´æ–°å†…å­˜ Schema â†’ åç»­æ–‡æ¡£ä½¿ç”¨æ–° Schema
```

---

## ğŸš€ **è¯¦ç»†å¤„ç†æ­¥éª¤**

### **Step 1: LLM å‘ç°æ–°ç±»å‹**

åœ¨å¤„ç†æ¯ä¸ªæ–‡æœ¬å—æ—¶ï¼ŒLLM å¯èƒ½è¿”å›æ–°çš„ schema ç±»å‹ï¼š

```python
# LLM è¿”å›ç¤ºä¾‹
{
  "attributes": {
    "Tesla Model 3": ["color: red", "price: $40,000"]
  },
  "triples": [
    ["Tesla Model 3", "manufactured_by", "Tesla Inc."],
    ["Tesla Model 3", "has_feature", "Autopilot"]
  ],
  "entity_types": {
    "Tesla Model 3": "vehicle",        # ğŸ†• æ–°å®ä½“ç±»å‹
    "Tesla Inc.": "organization",
    "Autopilot": "technology"          # ğŸ†• æ–°å®ä½“ç±»å‹
  },
  "new_schema_types": {                # ğŸŒŸ å…³é”®å­—æ®µ
    "nodes": ["vehicle", "technology"],
    "relations": ["manufactured_by", "has_feature"],
    "attributes": ["color", "price"]
  }
}
```

---

### **Step 2: è§£æå’ŒéªŒè¯**

```python
def process_level1_level2_agent(self, chunk: str, id: int):
    # 1. è°ƒç”¨ LLM
    llm_response = self.extract_with_llm(prompt)
    
    # 2. è§£æå“åº”
    parsed_response = self._validate_and_parse_llm_response(prompt, llm_response)
    if not parsed_response:
        return  # è§£æå¤±è´¥ï¼Œè·³è¿‡
    
    # 3. æå–æ–°ç±»å‹
    new_schema_types = parsed_response.get("new_schema_types", {})
    
    # 4. å¦‚æœæœ‰æ–°ç±»å‹ï¼Œæ›´æ–° schema
    if new_schema_types:
        self._update_schema_with_new_types(new_schema_types)
```

---

### **Step 3: å»é‡æ£€æŸ¥å’Œæ›´æ–° Schema æ–‡ä»¶**

```python
def _update_schema_with_new_types(self, new_schema_types: Dict[str, List[str]]):
    """æ›´æ–° Schema æ–‡ä»¶ï¼Œåªæ·»åŠ ä¸å­˜åœ¨çš„ç±»å‹"""
    
    # 1. ç¡®å®š schema æ–‡ä»¶è·¯å¾„
    schema_paths = {
        "hotpot": "schemas/hotpot.json",
        "2wiki": "schemas/2wiki.json",
        "musique": "schemas/musique.json",
        "demo": "schemas/demo.json",      # âš ï¸ æ³¨æ„ï¼šdemo ä¸åœ¨æ˜ å°„ä¸­
        "graphrag-bench": "schemas/graphrag-bench.json"
    }
    
    schema_path = schema_paths.get(self.dataset_name)
    if not schema_path:
        return  # âš ï¸ å¦‚æœæ•°æ®é›†ä¸åœ¨æ˜ å°„ä¸­ï¼Œä¸æ›´æ–°
    
    # 2. è¯»å–å½“å‰ schema
    with open(schema_path, 'r', encoding='utf-8') as f:
        current_schema = json.load(f)
    
    # å½“å‰ demo.json å†…å®¹ï¼š
    # {
    #   "Nodes": ["person", "location", "organization", ...],
    #   "Relations": ["is_a", "part_of", ...],
    #   "Attributes": ["name", "date", ...]
    # }
    
    updated = False
    
    # 3. æ·»åŠ æ–°çš„èŠ‚ç‚¹ç±»å‹ï¼ˆå»é‡ï¼‰
    if "nodes" in new_schema_types:
        for new_node in new_schema_types["nodes"]:  # ["vehicle", "technology"]
            if new_node not in current_schema.get("Nodes", []):
                current_schema.setdefault("Nodes", []).append(new_node)
                updated = True
                logger.info(f"âœ¨ Added new node type: {new_node}")
    
    # 4. æ·»åŠ æ–°çš„å…³ç³»ç±»å‹ï¼ˆå»é‡ï¼‰
    if "relations" in new_schema_types:
        for new_relation in new_schema_types["relations"]:  # ["manufactured_by", "has_feature"]
            if new_relation not in current_schema.get("Relations", []):
                current_schema.setdefault("Relations", []).append(new_relation)
                updated = True
                logger.info(f"âœ¨ Added new relation type: {new_relation}")
    
    # 5. æ·»åŠ æ–°çš„å±æ€§ç±»å‹ï¼ˆå»é‡ï¼‰
    if "attributes" in new_schema_types:
        for new_attribute in new_schema_types["attributes"]:  # ["color", "price"]
            if new_attribute not in current_schema.get("Attributes", []):
                current_schema.setdefault("Attributes", []).append(new_attribute)
                updated = True
                logger.info(f"âœ¨ Added new attribute type: {new_attribute}")
    
    # 6. å¦‚æœæœ‰æ›´æ–°ï¼Œä¿å­˜åˆ°æ–‡ä»¶
    if updated:
        with open(schema_path, 'w', encoding='utf-8') as f:
            json.dump(current_schema, f, ensure_ascii=False, indent=2)
        
        # 7. æ›´æ–°å†…å­˜ä¸­çš„ schema
        self.schema = current_schema
        
        logger.info(f"ğŸ“ Schema file updated: {schema_path}")
```

---

### **Step 4: æ›´æ–°åçš„ Schema ç¤ºä¾‹**

**æ›´æ–°å‰** (`schemas/demo.json`):
```json
{
  "Nodes": [
    "person", "location", "organization", "event", "object"
  ],
  "Relations": [
    "is_a", "part_of", "located_in", "created_by"
  ],
  "Attributes": [
    "name", "date", "size"
  ]
}
```

**æ›´æ–°å**:
```json
{
  "Nodes": [
    "person", "location", "organization", "event", "object",
    "vehicle",      // ğŸ†• æ–°å¢
    "technology"    // ğŸ†• æ–°å¢
  ],
  "Relations": [
    "is_a", "part_of", "located_in", "created_by",
    "manufactured_by",  // ğŸ†• æ–°å¢
    "has_feature"       // ğŸ†• æ–°å¢
  ],
  "Attributes": [
    "name", "date", "size",
    "color",   // ğŸ†• æ–°å¢
    "price"    // ğŸ†• æ–°å¢
  ]
}
```

---

### **Step 5: åç»­æ–‡æ¡£ä½¿ç”¨æ–° Schema**

æ›´æ–°åï¼Œæ–°çš„ schema ä¼šç«‹å³ç”Ÿæ•ˆï¼š

```python
# ä¸‹ä¸€ä¸ªæ–‡æœ¬å—å¤„ç†æ—¶
prompt = self._get_construction_prompt(next_chunk)

# Prompt ä¸­åŒ…å«æœ€æ–°çš„ schema
"""
Guidelines:
1. Prioritize the following predefined schema for extraction:
   ```
   {
     "Nodes": ["person", "location", ..., "vehicle", "technology"],
     "Relations": ["is_a", ..., "manufactured_by", "has_feature"],
     "Attributes": ["name", "date", ..., "color", "price"]
   }
   ```
2. Flexibility: If the context doesn't fit, extract as needed
...
"""
```

**ä¼˜ç‚¹**ï¼š
- âœ… åç»­æ–‡æ¡£æŠ½å–æ—¶ï¼ŒLLM èƒ½è¯†åˆ« "vehicle"ã€"technology" ç­‰æ–°ç±»å‹
- âœ… å‡å°‘é‡å¤å‘ç°ç›¸åŒç±»å‹
- âœ… Schema é€æ­¥å®Œå–„

---

## ğŸ” **å…³é”®æœºåˆ¶**

### **1. çº¿ç¨‹å®‰å…¨**
```python
with self.lock:
    self._process_attributes_agent(extracted_attr, id, entity_types)
    self._process_triples_agent(extracted_triples, id, entity_types)
```
- ä½¿ç”¨é”ä¿æŠ¤ schema æ›´æ–°
- é¿å…å¤šçº¿ç¨‹å¹¶å‘å†™å…¥å†²çª

---

### **2. å»é‡æœºåˆ¶**
```python
if new_node not in current_schema.get("Nodes", []):
    current_schema.setdefault("Nodes", []).append(new_node)
```
- ä¸¥æ ¼æ£€æŸ¥ï¼Œåªæ·»åŠ ä¸å­˜åœ¨çš„ç±»å‹
- é¿å… schema æ±¡æŸ“

---

### **3. å†…å­˜åŒæ­¥**
```python
self.schema = current_schema  # æ›´æ–°å†…å­˜ä¸­çš„ schema
```
- æ–‡ä»¶å’Œå†…å­˜ä¿æŒä¸€è‡´
- ç«‹å³å¯¹åç»­å¤„ç†ç”Ÿæ•ˆ

---

## âš ï¸ **é‡è¦é™åˆ¶**

### **1. æ•°æ®é›†ç™½åå•é™åˆ¶**

```python
schema_paths = {
    "hotpot": "schemas/hotpot.json",
    "2wiki": "schemas/2wiki.json",
    "musique": "schemas/musique.json",
    "novel": "schemas/novels_chs.json",
    "graphrag-bench": "schemas/graphrag-bench.json"
    # âš ï¸ æ³¨æ„ï¼šdemo ä¸åœ¨è¿™é‡Œï¼
}

schema_path = schema_paths.get(self.dataset_name)
if not schema_path:
    return  # é»˜è®¤æ•°æ®é›† demo ä¸ä¼šæ›´æ–° schema
```

**ç»“è®º**ï¼š
- âŒ **demo æ•°æ®é›†ä¸ä¼šæ›´æ–° schema æ–‡ä»¶**
- âŒ é€šè¿‡ Web ç•Œé¢ä¸Šä¼ çš„æ•°æ®é›†ï¼ˆé»˜è®¤ä½¿ç”¨ demo schemaï¼‰ä¹Ÿä¸ä¼šæ›´æ–°
- âœ… åªæœ‰ç™½åå•ä¸­çš„æ•°æ®é›†æ‰ä¼šæ›´æ–°

---

### **2. å¦‚ä½•ä¸ºæ–°æ•°æ®é›†å¯ç”¨ Schema Evolutionï¼Ÿ**

**æ–¹æ³•1: ä¿®æ”¹ä»£ç æ·»åŠ æ˜ å°„**

```python
# åœ¨ kt_gen.py ä¸­ä¿®æ”¹
schema_paths = {
    "hotpot": "schemas/hotpot.json",
    # ... å…¶ä»–æ•°æ®é›† ...
    "my_dataset": "schemas/my_dataset.json",  # æ·»åŠ ä½ çš„æ•°æ®é›†
}
```

**æ–¹æ³•2: åŠ¨æ€ç”Ÿæˆ schema è·¯å¾„**

```python
def _update_schema_with_new_types(self, new_schema_types: Dict[str, List[str]]):
    # æ”¹è¿›ç‰ˆæœ¬ï¼šè‡ªåŠ¨æŸ¥æ‰¾ schema æ–‡ä»¶
    schema_path = f"schemas/{self.dataset_name}.json"
    
    if not os.path.exists(schema_path):
        # å¦‚æœ schema ä¸å­˜åœ¨ï¼Œä½¿ç”¨ demo schema ä½œä¸ºæ¨¡æ¿
        schema_path = "schemas/demo.json"
    
    # åç»­å¤„ç†...
```

---

## ğŸ“ˆ **Schema Evolution çš„æ•ˆæœ**

### **æ„å»ºåˆæœŸ**
```json
{
  "Nodes": ["person", "location", "organization"],
  "Relations": ["located_in", "works_at"],
  "Attributes": ["name", "date"]
}
```

### **å¤„ç† 100 ä¸ªæ–‡æ¡£å**
```json
{
  "Nodes": [
    "person", "location", "organization",
    "vehicle", "technology", "product", "service",  // æ–°å¢
    "software", "hardware", "company_division"      // æ–°å¢
  ],
  "Relations": [
    "located_in", "works_at",
    "manufactured_by", "founded_by", "acquired_by",  // æ–°å¢
    "competes_with", "partners_with"                 // æ–°å¢
  ],
  "Attributes": [
    "name", "date",
    "price", "color", "version", "release_date",     // æ–°å¢
    "market_cap", "employee_count"                   // æ–°å¢
  ]
}
```

---

## ğŸ¯ **å®é™…åº”ç”¨åœºæ™¯**

### **åœºæ™¯1: å¤„ç†ç§‘æŠ€æ–°é—»**

**æ–‡æ¡£1** (åˆå§‹ schema):
```
Tesla announces new Model Y...
```
**å‘ç°**: `vehicle`, `electric_car` â†’ æ·»åŠ åˆ° schema

**æ–‡æ¡£2** (ä½¿ç”¨æ›´æ–°åçš„ schema):
```
Apple launches iPhone 15...
```
**å‘ç°**: LLM å·²çŸ¥ `product` ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨
**æ–°å‘ç°**: `smartphone`, `mobile_device` â†’ ç»§ç»­æ·»åŠ 

---

### **åœºæ™¯2: å¤„ç†å­¦æœ¯è®ºæ–‡**

**è®ºæ–‡1**:
```
Neural networks for image classification...
```
**å‘ç°**: `algorithm`, `model`, `dataset` â†’ æ·»åŠ 

**è®ºæ–‡2**:
```
Transformer architecture improves NLP tasks...
```
**å‘ç°**: LLM å·²çŸ¥ `algorithm`, `model`
**æ–°å‘ç°**: `architecture`, `benchmark` â†’ æ·»åŠ 

---

## ğŸ’¡ **æœ€ä½³å®è·µ**

### **1. è®¾ç½®ä¸¥æ ¼çš„æ·»åŠ é˜ˆå€¼**

LLM Prompt ä¸­å·²åŒ…å«ï¼š
```
Notably, the strict threshold of adding new schema 
considering both importance and similarity to the 
pattern in the existing schema is 0.9.
```

è¿™ç¡®ä¿åªæœ‰**çœŸæ­£é‡è¦ä¸”ä¸é‡å¤**çš„ç±»å‹æ‰ä¼šè¢«æ·»åŠ ã€‚

---

### **2. å®šæœŸå®¡æŸ¥ Schema**

```python
# æŸ¥çœ‹ schema æ¼”åŒ–å†å²
git log schemas/hotpot.json

# æ‰‹åŠ¨æ¸…ç†ä¸å¿…è¦çš„ç±»å‹
# ç¼–è¾‘ schemas/hotpot.jsonï¼Œåˆ é™¤å†—ä½™ç±»å‹
```

---

### **3. ä½¿ç”¨ Schema ç‰ˆæœ¬æ§åˆ¶**

```python
{
  "version": "1.2.3",
  "last_updated": "2024-01-15",
  "Nodes": [...],
  "Relations": [...],
  "Attributes": [...]
}
```

---

## ğŸ“Š **æ€»ç»“å¯¹æ¯”**

| æ–¹é¢ | Agent æ¨¡å¼ | NoAgent æ¨¡å¼ |
|------|-----------|-------------|
| Schema æ›´æ–° | âœ… åŠ¨æ€æ›´æ–°æ–‡ä»¶ | âŒ ä¸æ›´æ–° |
| æ–°ç±»å‹å¤„ç† | âœ… æ·»åŠ åˆ° schema | âŒ å¿½ç•¥æˆ–å¼ºåˆ¶æ˜ å°„åˆ°ç°æœ‰ç±»å‹ |
| é€‚åº”æ€§ | âœ… é«˜ï¼ˆè‡ªåŠ¨å­¦ä¹ ï¼‰ | âŒ ä½ï¼ˆå›ºå®š schemaï¼‰ |
| å¯æ§æ€§ | âš ï¸ ä¸­ï¼ˆéœ€è¦é˜ˆå€¼æ§åˆ¶ï¼‰ | âœ… é«˜ï¼ˆå®Œå…¨å¯æ§ï¼‰ |
| Schema ä¸€è‡´æ€§ | âš ï¸ å¯èƒ½æ¼”åŒ–å¾—å¾ˆå¤§ | âœ… å§‹ç»ˆä¸€è‡´ |
| é€‚ç”¨åœºæ™¯ | å¼€æ”¾åŸŸã€æ¢ç´¢æ€§ä»»åŠ¡ | å°é—­åŸŸã€æ ‡å‡†åŒ–ä»»åŠ¡ |

---

## ğŸ”§ **å®æˆ˜å»ºè®®**

1. **é¦–æ¬¡æ„å»º**: ä½¿ç”¨ **agent æ¨¡å¼**ï¼Œè®©ç³»ç»Ÿå­¦ä¹ é¢†åŸŸ schema
2. **Schema ç¨³å®šå**: åˆ‡æ¢åˆ° **noagent æ¨¡å¼**ï¼Œä¿æŒä¸€è‡´æ€§
3. **å®šæœŸå®¡æŸ¥**: æ£€æŸ¥ schema æ¼”åŒ–æ˜¯å¦åˆç†
4. **å¤‡ä»½ schema**: æ„å»ºå‰å¤‡ä»½åŸå§‹ schema æ–‡ä»¶

```bash
# æ„å»ºå‰å¤‡ä»½
cp schemas/demo.json schemas/demo.json.backup

# æ„å»ºåå¯¹æ¯”
diff schemas/demo.json schemas/demo.json.backup
```

è¿™å°±æ˜¯ Agent æ¨¡å¼ä¸‹æ–°å‘ç°å®ä½“å’Œå…³ç³»çš„å®Œæ•´å¤„ç†æµç¨‹ï¼ğŸ‰

---

## æ€»ç»“

æœ¬æ¬¡å¯¹è¯è¯¦ç»†æ¢³ç†äº† Youtu-GraphRAG ç³»ç»Ÿçš„å®Œæ•´å·¥ä½œæµç¨‹ï¼š

1. **çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹**ï¼šä» Web ç•Œé¢ä¸Šä¼ æ•°æ®ã€ç‚¹å‡» construct åçš„å®Œæ•´å¤„ç†æµç¨‹
2. **Agent vs NoAgent æ¨¡å¼**ï¼šä¸¤ç§æ¨¡å¼çš„åŒºåˆ«ã€ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯
3. **æ£€ç´¢ä¸é—®ç­”æµç¨‹**ï¼šé—®é¢˜åˆ†è§£ã€å¤šè·¯å¬å›ã€IRCoT è¿­ä»£æ¨ç†çš„è¯¦ç»†å®ç°
4. **Schema Evolution æœºåˆ¶**ï¼šAgent æ¨¡å¼ä¸‹å¦‚ä½•åŠ¨æ€å‘ç°å’Œæ›´æ–° schema ç±»å‹

æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹ï¼š
- å¹¶å‘å¤„ç†ï¼ˆThreadPoolExecutorï¼‰
- LLM é©±åŠ¨çš„ä¿¡æ¯æŠ½å–
- å¤šå±‚å›¾ç»“æ„ï¼ˆLevel 1-4ï¼‰
- FastTreeComm ç¤¾åŒºæ£€æµ‹
- FAISS å‘é‡ç´¢å¼•
- IRCoT è¿­ä»£æ¨ç†
- Schema åŠ¨æ€æ¼”åŒ–

å¸Œæœ›è¿™ä»½å¯¹è¯è®°å½•èƒ½å¸®åŠ©ä½ æ·±å…¥ç†è§£ Youtu-GraphRAG çš„è®¾è®¡ç†å¿µå’Œå®ç°ç»†èŠ‚ï¼
